logging:
  path_base: '/'
  unique_string: 'PLACEHOLDER'

dataset:
  name: amazon
  path: datasets/TextClassification/amazon

reproduce:  # seed for reproduction 
  seed: 0  # a seed for all random part, PLACEHOLDER

plm:
  model_name: gpt2_style_lm
  model_path: PLACEHOLDER
  optimize:
    freeze_para: True # False, True
    # lr: 0.00003
    # weight_decay: 0.01

dataloader:
  max_seq_length: 256  # max_seq_length
  decoder_max_length: 3 # the decoder max length to truncate decoder input sequence
                    # if it is an encoder-decoder architecture. Note that it's not equavalent
                    # to generation.max_length which is used merely in the generation phase.
  truncate_method: "head" # choosing from balanced, head, tail
  decode_from_pad: true

train:
  batch_size: 2
  gradient_accumulation_steps: 8
  batch_size: 4
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  num_epochs:
  num_training_steps: 10000


test:
  batch_size: 16

dev:
  batch_size: 16


template: soft_template
verbalizer: manual_verbalizer



soft_template:
  choice: 2 # 0 for head prompt setup, 1 for tail prompt setup
  prompt_position: head # head for head prompt setup, tail for tail prompt setup
  prompt_text: "pmwiki In The song Simpsons"
  file_path: my_scripts/TextClassification/amazon/soft_template.txt
  num_tokens: 10000 # just a placeholder
  initialize_from_vocab: true # this is disabled, default to uniform
  random_range: 1.0
  optimize: 
    name: AdamW
    lr: -100 # placeholder
    adam_epsilon: 1.0e-8
    scheduler:
      num_warmup_steps: 0


manual_verbalizer:
  choice: 0
  file_path: my_scripts/TextClassification/amazon/manual_verbalizer.txt
  
environment:
  num_gpus: 1
  cuda_visible_devices:
  local_rank: 0
  model_parallel: False
  device_map:

learning_setting: zero_shot

task: classification
classification:
  parent_config: task
  metric:  # the first one will be the main to determine checkpoint.
    - accuracy
    - precision
    - recall
#    - micro-f1
#    - accuracy
  loss_function: cross_entropy ## the loss function for classification


#sampling_from_train:
#  parent_config: few_shot_sampling
#  num_examples_per_label: 5000
#  also_sample_dev: False # num_examples_per_label_dev: 10000
#  seed: # can list multiple seeds
#    - 2022